{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "class MySentence():\n",
    "    def __init__(self,processedWords, originalWords):\n",
    "        self.processedWords = processedWords\n",
    "        self.wordFrequencies = self.sentenceWordFreq()\n",
    "        self.originalWords = originalWords\n",
    "\n",
    "    def getProcessedWords(self):\n",
    "        return self.processedWords\n",
    "\n",
    "    def getOriginalWords(self):\n",
    "        return self.originalWords\n",
    "\n",
    "    def getWordFreq(self):\n",
    "        return self.wordFrequencies\t\n",
    "\n",
    "    def sentenceWordFreq(self):\n",
    "        wordFreq = {}\n",
    "        for word in self.processedWords:\n",
    "            if word not in wordFreq.keys():\n",
    "                wordFreq[word] = 1\n",
    "            else:\n",
    "                if word in stopwords.words('english'):\n",
    "                    wordFreq[word] = 1\n",
    "                else:\n",
    "                    wordFreq[word] = wordFreq[word] + 1\n",
    "        return wordFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.bbc_gpt2_all import gpt2\n",
    "from ipynb.fs.defs.bbc_bert1_all import bert\n",
    "from ipynb.fs.defs.bbc_bart1_all import bart\n",
    "from ipynb.fs.defs.bbc_gpt2_all import gpt2\n",
    "from ipynb.fs.defs.bbc_bert1_all import bert\n",
    "from ipynb.fs.defs.bbc_bart1_all import bart\n",
    "from ipynb.fs.defs.multiple import cosine_similarity\n",
    "from ipynb.fs.defs.multiple import jaccard_similarity\n",
    "from ipynb.fs.defs.multiple import get_processed_text\n",
    "from ipynb.fs.defs.speech_to_text import get_text\n",
    "from ipynb.fs.defs.text_to_speech import convert_to_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFile(text):\n",
    "    sentence_token = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    lines=[]\n",
    "    sent=\"\"\n",
    "    for char in text:\n",
    "        if(char=='.'):\n",
    "            lines.append(sent)\n",
    "            sent=\"\"\n",
    "        else:\n",
    "            sent=sent+char\n",
    "        prev=char\n",
    "    sentences = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "    for line in lines:\n",
    "        originalWords = line[:]\n",
    "        line = line.strip().lower()\n",
    "        sent = nltk.word_tokenize(line)\n",
    "        stemmedSent = [porter.stem(word) for word in sent]\n",
    "        for x in stemmedSent:\n",
    "                if(x=='.'or x=='`'or x==','or x=='?'or x==\"'\" or x=='!' or x=='''\"''' or x==\"''\" or x==\"'s\"):\n",
    "                    stemmedSent.remove(x)\n",
    "        if stemmedSent != []:\n",
    "            sent=MySentence(stemmedSent,originalWords)\n",
    "            sentences.append(sent)            \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.MySentence'>\n",
      "processedWords  ['from', 'bbc', 'learn', 'english'] \n",
      "\n",
      "Original words  from BBC learning English  \n",
      "\n",
      "Sentence Frequenices  {'from': 1, 'bbc': 1, 'learn': 1, 'english': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['thi', 'is', 'a', 'download', 'from', 'bbc', 'learn', 'english', 'to', 'find', 'out', 'more', 'visit', 'our', 'websit'] \n",
      "\n",
      "Original words  this is a download from BBC learning English to find out more visit our website  \n",
      "\n",
      "Sentence Frequenices  {'thi': 1, 'is': 1, 'a': 1, 'download': 1, 'from': 1, 'bbc': 1, 'learn': 1, 'english': 1, 'to': 1, 'find': 1, 'out': 1, 'more': 1, 'visit': 1, 'our': 1, 'websit': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['from', 'bbc', 'learn', 'english', 'dot', 'com'] \n",
      "\n",
      "Original words  from BBC learning English dot com  \n",
      "\n",
      "Sentence Frequenices  {'from': 1, 'bbc': 1, 'learn': 1, 'english': 1, 'dot': 1, 'com': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['hello', 'thi', 'is', 'six', 'minut', 'english', 'from', 'bbc', 'learn', 'english', 'i', \"'m\", 'nail', 'and', 'i', \"'m\", 'georgina', 'what', 'do', 'u'] \n",
      "\n",
      "Original words  hello this is six minutes English from BBC learning English I'm nail and I'm Georgina what do U \n",
      "\n",
      "Sentence Frequenices  {'hello': 1, 'thi': 1, 'is': 1, 'six': 1, 'minut': 1, 'english': 2, 'from': 1, 'bbc': 1, 'learn': 1, 'i': 1, \"'m\": 2, 'nail': 1, 'and': 1, 'georgina': 1, 'what': 1, 'do': 1, 'u': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['homer', 'rachel', 'and', 'jorg', 'bought', 'ha', 'all', 'have', 'in', 'common', 'to', 'a', 'gina'] \n",
      "\n",
      "Original words   Homer Rachel's and Jorge bought has all have in common to a Gina  \n",
      "\n",
      "Sentence Frequenices  {'homer': 1, 'rachel': 1, 'and': 1, 'jorg': 1, 'bought': 1, 'ha': 1, 'all': 1, 'have': 1, 'in': 1, 'common': 1, 'to': 1, 'a': 1, 'gina': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['so', 'that', 'the', 'agent', 'greek', 'poet', 'homer', 'american', 'singer', 'ray', 'charl', 'and', 'argentin', 'writer', 'jorg', 'lui', 'borg', 'i', 'ca', \"n't\", 'see', 'much', 'in', 'common', 'they', 'kneel', 'well', 'the', 'answer', 'is', 'that', 'they', 'were', 'all', 'blind', 'but', 'that', 'obvious', 'did', \"n't\", 'hold', 'them', 'back', 'i', 'mean', 'there', 'were', 'some', 'of', 'the', 'greatest', 'artist', 'ever', 'right', 'but', 'i', 'wonder', 'how', 'easi', 'they', 'would', 'find', 'it', 'live', 'and', 'work', 'in', 'the', 'modern', 'world', 'tri', 'peopl', 'can', 'use', 'a', 'guy', 'dog', 'or', 'a', 'white', 'came', 'to', 'help', 'them', 'move', 'around', 'ye', 'but', 'it', 'wo', \"n't\", 'kane', 'is', 'hardli', 'advanc', 'technolog', 'recent', 'smartphon', 'app', 'have', 'been', 'invent', 'which', 'dramat', 'improv', 'the', 'live', 'of', 'blind', 'peopl', 'around', 'the', 'world', 'in', 'thi', 'programm', 'on', 'blind', 'in', 'the', 'digit', 'age', 'will', 'be', 'look', 'at', 'some', 'of', 'these', 'invent', 'known', 'collect', 'as', 'assist', 'technolog', 'that', 'ani', 'softwar', 'or', 'equip', 'that', 'help', 'peopl', 'work', 'around', 'that', 'disabl', 'or', 'challeng', 'but', 'first', 'it', 'time', 'for', 'my', 'quick', 'question', 'georgina', 'in', 'eighteen', 'forti', 'two', 'a', 'techniqu', 'of', 'use', 'finger', 'to', 'fill', 'print', 'rais', 'dot', 'wa', 'invent', 'which', 'allow', 'blind', 'peopl', 'to', 'read', 'but', 'who', 'invent', 'it', 'wa', 'it', 'a', 'margaret', 'walker', 'b'] \n",
      "\n",
      "Original words  so that's the agent Greek poet Homer American singer Ray Charles and Argentine writer Jorge Luis Borges I can't see much in common they kneel well the answer is that they were all blind but that obviously didn't hold them back I mean there were some of the greatest artists ever right but I wonder how easy they would find it living and working in the modern world trying people can use a guy dog or a white came to help them move around yes but it won't Kane is hardly advanced technology recently smartphone apps have been invented which dramatically improve the lives of blind people around the world in this programme on blindness in the digital age will be looking at some of these inventions known collectively as assistive technology that's any software or equipment that helps people work around that disabilities or challenges but first it's time for my quick question Georgina in eighteen forty two a technique of using fingers to fill printed raised dots was invented which allowed blind people to read but who invented it was it a Margaret Walker B \n",
      "\n",
      "Sentence Frequenices  {'so': 1, 'that': 1, 'the': 1, 'agent': 1, 'greek': 1, 'poet': 1, 'homer': 1, 'american': 1, 'singer': 1, 'ray': 1, 'charl': 1, 'and': 1, 'argentin': 1, 'writer': 1, 'jorg': 1, 'lui': 1, 'borg': 1, 'i': 1, 'ca': 1, \"n't\": 3, 'see': 1, 'much': 1, 'in': 1, 'common': 1, 'they': 1, 'kneel': 1, 'well': 1, 'answer': 1, 'is': 1, 'were': 1, 'all': 1, 'blind': 4, 'but': 1, 'obvious': 1, 'did': 1, 'hold': 1, 'them': 1, 'back': 1, 'mean': 1, 'there': 1, 'some': 1, 'of': 1, 'greatest': 1, 'artist': 1, 'ever': 1, 'right': 1, 'wonder': 1, 'how': 1, 'easi': 1, 'would': 1, 'find': 1, 'it': 1, 'live': 2, 'work': 2, 'modern': 1, 'world': 2, 'tri': 1, 'peopl': 4, 'can': 1, 'use': 2, 'a': 1, 'guy': 1, 'dog': 1, 'or': 1, 'white': 1, 'came': 1, 'to': 1, 'help': 2, 'move': 1, 'around': 3, 'ye': 1, 'wo': 1, 'kane': 1, 'hardli': 1, 'advanc': 1, 'technolog': 2, 'recent': 1, 'smartphon': 1, 'app': 1, 'have': 1, 'been': 1, 'invent': 4, 'which': 1, 'dramat': 1, 'improv': 1, 'thi': 1, 'programm': 1, 'on': 1, 'digit': 1, 'age': 1, 'will': 1, 'be': 1, 'look': 1, 'at': 1, 'these': 1, 'known': 1, 'collect': 1, 'as': 1, 'assist': 1, 'ani': 1, 'softwar': 1, 'equip': 1, 'disabl': 1, 'challeng': 1, 'first': 1, 'time': 1, 'for': 1, 'my': 1, 'quick': 1, 'question': 1, 'georgina': 1, 'eighteen': 1, 'forti': 1, 'two': 1, 'techniqu': 1, 'finger': 1, 'fill': 1, 'print': 1, 'rais': 1, 'dot': 1, 'wa': 2, 'allow': 1, 'read': 1, 'who': 1, 'margaret': 1, 'walker': 1, 'b': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['loui', 'braill', 'or', 'c'] \n",
      "\n",
      "Original words   Louis braille or C \n",
      "\n",
      "Sentence Frequenices  {'loui': 1, 'braill': 1, 'or': 1, 'c': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['samuel', 'mors', 'i', \"'ve\", 'heard', 'of', 'lost', 'code', 'but', 'that', 'would', \"n't\", 'help', 'blind', 'peopl', 'read', 'so', 'i', 'think', 'it', 'be', 'loui', 'braill', 'okay', 'georgina', 'will', 'find', 'out', 'the', 'answer', 'at', 'the', 'end', 'of', 'the', 'programm', 'one', 'remark', 'featur', 'of', 'the', 'latest', 'assist', 'technolog', 'is', 'it', 'practic', 'smartphon', 'app', 'like', 'be', 'my', 'eye', 'allow', 'blind', 'user', 'to', 'find', 'lost', 'key', 'cross', 'busi', 'road', 'and', 'even', 'colour', 'match', 'their', 'cloth', 'prior', 'no', 'wonder', 'is', 'ceo', 'of', 'a', 'canyon', 'compani', 'develop', 'thi', 'kind', 'of', 'technolog', 'here', 'he', 'explain', 'to', 'bbc', 'world', 'servic', 'programm', 'digit', 'planet', 'how', 'hi', 'devic', 'situat', 'han', 'not', 'replac', 'the', 'tradit', 'white', 'came'] \n",
      "\n",
      "Original words   Samuel Morse I've heard of lost code but that wouldn't help blind people read so I think it's being Louis braille okay Georgina will find out the answer at the end of the programme one remarkable feature of the latest assistive technology is its practicality smartphone apps like be my eyes allow blind users to find lost keys cross busy roads and even colour match their clothes prior no wonder is CEO of a canyon company developing this kind of technology here he explained to BBC world service programme digital planet how his devices situation Hans not replaced the traditional white came  \n",
      "\n",
      "Sentence Frequenices  {'samuel': 1, 'mors': 1, 'i': 1, \"'ve\": 1, 'heard': 1, 'of': 1, 'lost': 2, 'code': 1, 'but': 1, 'that': 1, 'would': 1, \"n't\": 1, 'help': 1, 'blind': 2, 'peopl': 1, 'read': 1, 'so': 1, 'think': 1, 'it': 1, 'be': 1, 'loui': 1, 'braill': 1, 'okay': 1, 'georgina': 1, 'will': 1, 'find': 2, 'out': 1, 'the': 1, 'answer': 1, 'at': 1, 'end': 1, 'programm': 2, 'one': 1, 'remark': 1, 'featur': 1, 'latest': 1, 'assist': 1, 'technolog': 2, 'is': 1, 'practic': 1, 'smartphon': 1, 'app': 1, 'like': 1, 'my': 1, 'eye': 1, 'allow': 1, 'user': 1, 'to': 1, 'key': 1, 'cross': 1, 'busi': 1, 'road': 1, 'and': 1, 'even': 1, 'colour': 1, 'match': 1, 'their': 1, 'cloth': 1, 'prior': 1, 'no': 1, 'wonder': 1, 'ceo': 1, 'a': 1, 'canyon': 1, 'compani': 1, 'develop': 1, 'thi': 1, 'kind': 1, 'here': 1, 'he': 1, 'explain': 1, 'bbc': 1, 'world': 1, 'servic': 1, 'digit': 1, 'planet': 1, 'how': 1, 'hi': 1, 'devic': 1, 'situat': 1, 'han': 1, 'not': 1, 'replac': 1, 'tradit': 1, 'white': 1, 'came': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['the', 'devic', 'is', 'veri', 'compat', 'with', 'ani', 'kind', 'of', 'white', 'again', 'so', 'onc', 'you', 'click', 'on', 'it', 'anyway', 'can', 'i', 'to', 'work', 'perfectli', 'to', 'detect', 'the', 'obstacl', 'in', 'front', 'of', 'you', 'as', 'usual', 'lie', 'on', 'echo', 'locat', 'so', 'i', 'can', 'locat', 'is', 'the', 'same', 'technolog', 'use', 'by', 'either', 'but', 'and', 'dolphin', 'to', 'detect', 'prey', 'and', 'obstacl', 'and', 'all', 'that', 'you', 'send', 'out', 'our', 'sound', 'fault', 'and', 'then', 'onc', 'it', 'both', 'off', 'and', 'also', 'quot', 'you', 'can', 'tell', 'how', 'far', 'the', 'other', 'polici', 'when', 'attach', 'to', 'a', 'white', 'cane', 'the', 'digit', 'devic', 'call', 'six', 'cent', 'can', 'detect', 'obstacl', 'object', 'which', 'block', 'your', 'way', 'make', 'it', 'difficult', 'for', 'you', 'to', 'move', 'forward', 'six', 'cent', 'work', 'use', 'echo', 'locat', 'a', 'kind', 'of', 'ultra', 'sound', 'like', 'that', 'use', 'by', 'back', 'who', 'send', 'out', 'sound', 'way', 'which', 'balanc', 'off', 'surround', 'object', 'the', 'return', 'echo', 'show', 'where', 'these', 'object', 'are', 'locat', 'some', 'of', 'the', 'assist', 'app', 'are', 'so', 'smart', 'they', 'can', 'even', 'tell', 'what', 'kind', 'of', 'object', 'is', 'come', 'up', 'ahead', 'but', 'a', 'friend', 'a', 'shop', 'door', 'or', 'a', 'speed', 'car', 'i', 'guess', 'be', 'abl', 'to', 'move', 'around', 'confid', 'realli', 'boost', 'peopl', 'independ', 'absolut', 'and', 'it', 'challeng', 'stereotyp', 'around', 'blind', 'to', 'log', 'out', 'phone', 'a', 'lower', 'who', 'is', 'blind', 'herself', 'use', 'assist', 'app', 'everyday', 'here', 'she', 'is', 'talk', 'to', 'bbc', 'world', 'servic', 'digit', 'planet'] \n",
      "\n",
      "Original words  the device is very compatible with any kind of white again so once you click on it anyways can I to work perfectly to detect the obstacles in front of you as usual lies on echo location so I can location is the same technology used by either but and dolphins to detect prey and obstacles and all that you send out our sound faults and then once it both off and also quote you can tell how far the other policies when attached to a white cane the digital device called six cents can detect obstacles objects which block your way making it difficult for you to move forward six cents works using echo location a kind of ultra sound like that used by back who send out sound ways which balance off surrounding objects the returning echoes show where these objects are located some of the assistive apps are so smart they can even tell what kind of object is coming up ahead but a friend a shop door or a speeding car I guess being able to move around confidently really boost people's independence absolutely and it's challenging stereotypes around blindness to log out phone a lower who is blind herself uses assistive apps everyday here she is talking to BBC world service's digital planet  \n",
      "\n",
      "Sentence Frequenices  {'the': 1, 'devic': 2, 'is': 1, 'veri': 1, 'compat': 1, 'with': 1, 'ani': 1, 'kind': 3, 'of': 1, 'white': 2, 'again': 1, 'so': 1, 'onc': 2, 'you': 1, 'click': 1, 'on': 1, 'it': 1, 'anyway': 1, 'can': 1, 'i': 1, 'to': 1, 'work': 2, 'perfectli': 1, 'detect': 3, 'obstacl': 3, 'in': 1, 'front': 1, 'as': 1, 'usual': 1, 'lie': 1, 'echo': 3, 'locat': 4, 'same': 1, 'technolog': 1, 'use': 4, 'by': 1, 'either': 1, 'but': 1, 'and': 1, 'dolphin': 1, 'prey': 1, 'all': 1, 'that': 1, 'send': 2, 'out': 1, 'our': 1, 'sound': 3, 'fault': 1, 'then': 1, 'both': 1, 'off': 1, 'also': 1, 'quot': 1, 'tell': 2, 'how': 1, 'far': 1, 'other': 1, 'polici': 1, 'when': 1, 'attach': 1, 'a': 1, 'cane': 1, 'digit': 2, 'call': 1, 'six': 2, 'cent': 2, 'object': 4, 'which': 1, 'block': 1, 'your': 1, 'way': 2, 'make': 1, 'difficult': 1, 'for': 1, 'move': 2, 'forward': 1, 'ultra': 1, 'like': 1, 'back': 1, 'who': 1, 'balanc': 1, 'surround': 1, 'return': 1, 'show': 1, 'where': 1, 'these': 1, 'are': 1, 'some': 1, 'assist': 2, 'app': 2, 'smart': 1, 'they': 1, 'even': 1, 'what': 1, 'come': 1, 'up': 1, 'ahead': 1, 'friend': 1, 'shop': 1, 'door': 1, 'or': 1, 'speed': 1, 'car': 1, 'guess': 1, 'be': 1, 'abl': 1, 'around': 2, 'confid': 1, 'realli': 1, 'boost': 1, 'peopl': 1, 'independ': 1, 'absolut': 1, 'challeng': 1, 'stereotyp': 1, 'blind': 2, 'log': 1, 'phone': 1, 'lower': 1, 'herself': 1, 'everyday': 1, 'here': 1, 'she': 1, 'talk': 1, 'bbc': 1, 'world': 1, 'servic': 1, 'planet': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['i', 'think', 'the', 'more', 'that', 'societi', 'see', 'blind', 'peopl', 'in', 'the', 'commun', 'at', 'work', 'in', 'relationship', 'it', 'doe', 'help', 'to', 'tackl', 'all', 'of', 'these', 'stereotyp', 'it', 'help', 'to', 'peopl', 'for', 'feedback', 'blind', 'and', 'visual', 'impair', 'peopl', 'in', 'a', 'whole', 'new', 'way', 'and', 'it', 'just', 'normalis', 'disabl', 'that', 'what', 'we', 'need', 'we', 'need', 'to', 'see', 'peopl', 'just', 'get', 'on', 'with', 'their', 'life', 'and', 'do', 'it', 'and', 'then', 'peopl', 'wo', \"n't\", 'see', 'a', 'such', 'a', 'big', 'deal', 'anymor', 'i', \"'ll\", 'just', 'be', 'the', 'ordinari'] \n",
      "\n",
      "Original words  I think the more that society sees blind people in the community at work in relationships it does help to tackle all of these stereotypes it helps to people for feedback blind and visually impaired people in a whole new way and it just normalises disability that's what we need we need to see people just getting on with their life and doing it and then people won't see a such a big deal anymore I'll just be the ordinary  \n",
      "\n",
      "Sentence Frequenices  {'i': 1, 'think': 1, 'the': 1, 'more': 1, 'that': 1, 'societi': 1, 'see': 3, 'blind': 2, 'peopl': 5, 'in': 1, 'commun': 1, 'at': 1, 'work': 1, 'relationship': 1, 'it': 1, 'doe': 1, 'help': 2, 'to': 1, 'tackl': 1, 'all': 1, 'of': 1, 'these': 1, 'stereotyp': 1, 'for': 1, 'feedback': 1, 'and': 1, 'visual': 1, 'impair': 1, 'a': 1, 'whole': 1, 'new': 1, 'way': 1, 'just': 1, 'normalis': 1, 'disabl': 1, 'what': 1, 'we': 1, 'need': 2, 'get': 1, 'on': 1, 'with': 1, 'their': 1, 'life': 1, 'do': 1, 'then': 1, 'wo': 1, \"n't\": 1, 'such': 1, 'big': 1, 'deal': 1, 'anymor': 1, \"'ll\": 1, 'be': 1, 'ordinari': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['further', 'distinguish', 'between', 'peopl', 'who', 'are', 'blind', 'or', 'unabl', 'to', 'see', 'i', \"'m\", 'those', 'who', 'are', 'visual', 'impair', 'experi', 'a', 'decreas', 'abil', 'to', 'see'] \n",
      "\n",
      "Original words  further distinguishes between people who are blind or unable to see I'm those who are visually impaired experience a decreased ability to see  \n",
      "\n",
      "Sentence Frequenices  {'further': 1, 'distinguish': 1, 'between': 1, 'peopl': 1, 'who': 1, 'are': 1, 'blind': 1, 'or': 1, 'unabl': 1, 'to': 1, 'see': 2, 'i': 1, \"'m\": 1, 'those': 1, 'visual': 1, 'impair': 1, 'experi': 1, 'a': 1, 'decreas': 1, 'abil': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['east', 'of', 'tech', 'help', 'blind', 'peopl', 'leav', 'normal', 'independ', 'live', 'within', 'their', 'local', 'commun', 'fern', 'hope', 'that', 'thi', 'will', 'help', 'normalis', 'disabl', 'treat', 'someth', 'as', 'normal', 'which', 'ha', 'not', 'been', 'accept', 'as', 'normal', 'befor', 'so', 'be', 'blind', 'it', 'doe', \"n't\", 'have', 'to', 'be', 'a', 'big', 'deal', 'an', 'inform', 'way', 'to', 'say', 'someth', 'is', 'not', 'a', 'seriou', 'problem', 'just', 'keep', 'your', 'eye', 'close', 'for', 'a', 'minut', 'and', 'tri', 'move', 'around', 'the', 'room', 'you', \"'ll\", 'soon', 'see', 'how', 'difficult', 'it', 'is', 'and', 'how', 'life', 'chang', 'thi', 'technolog', 'can', 'be', 'be', 'abl', 'to', 'read', 'book', 'must', 'also', 'open', 'up', 'a', 'world', 'of', 'imagin', 'so', 'what', 'wa', 'the', 'answer', 'to', 'your', 'question', 'now', '%', 'hesit', 'ye', 'i', 'ask', 'georgina', 'who', 'invent', 'the', 'system', 'of', 'read', 'west', 'finger', 'tip', 'are', 'use', 'to', 'fill', 'pattern', 'of', 'print', 'rais', 'dot', 'what', 'did', 'you', 'say', 'georgina', 'i', 'thought', 'it', 'wa', 'b'] \n",
      "\n",
      "Original words  east of tech help blind people leave normal independent lives within their local communities fern hopes that this will help normalise disability treat something as normal which has not been accepted as normal before so being blind it doesn't have to be a big deal an informal way to say something is not a serious problem just keep your eyes closed for a minute and try moving around the room you'll soon see how difficult it is and how life changing this technology can be being able to read books must also open up a world of imagination so what was the answer to your question now %HESITATION yes I asked Georgina who invented the system of reading west finger tips are used to fill patterns of printed raise dot what did you say Georgina I thought it was B \n",
      "\n",
      "Sentence Frequenices  {'east': 1, 'of': 1, 'tech': 1, 'help': 2, 'blind': 2, 'peopl': 1, 'leav': 1, 'normal': 3, 'independ': 1, 'live': 1, 'within': 1, 'their': 1, 'local': 1, 'commun': 1, 'fern': 1, 'hope': 1, 'that': 1, 'thi': 2, 'will': 1, 'normalis': 1, 'disabl': 1, 'treat': 1, 'someth': 2, 'as': 1, 'which': 1, 'ha': 1, 'not': 1, 'been': 1, 'accept': 1, 'befor': 1, 'so': 1, 'be': 1, 'it': 1, 'doe': 1, \"n't\": 1, 'have': 1, 'to': 1, 'a': 1, 'big': 1, 'deal': 1, 'an': 1, 'inform': 1, 'way': 1, 'say': 2, 'is': 1, 'seriou': 1, 'problem': 1, 'just': 1, 'keep': 1, 'your': 1, 'eye': 1, 'close': 1, 'for': 1, 'minut': 1, 'and': 1, 'tri': 1, 'move': 1, 'around': 1, 'the': 1, 'room': 1, 'you': 1, \"'ll\": 1, 'soon': 1, 'see': 1, 'how': 1, 'difficult': 1, 'life': 1, 'chang': 1, 'technolog': 1, 'can': 1, 'abl': 1, 'read': 2, 'book': 1, 'must': 1, 'also': 1, 'open': 1, 'up': 1, 'world': 1, 'imagin': 1, 'what': 1, 'wa': 2, 'answer': 1, 'question': 1, 'now': 1, '%': 1, 'hesit': 1, 'ye': 1, 'i': 1, 'ask': 1, 'georgina': 2, 'who': 1, 'invent': 1, 'system': 1, 'west': 1, 'finger': 1, 'tip': 1, 'are': 1, 'use': 1, 'fill': 1, 'pattern': 1, 'print': 1, 'rais': 1, 'dot': 1, 'did': 1, 'thought': 1, 'b': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['loui', 'braill'] \n",
      "\n",
      "Original words   Louis braille  \n",
      "\n",
      "Sentence Frequenices  {'loui': 1, 'braill': 1} \n",
      "\n",
      "<class '__main__.MySentence'>\n",
      "processedWords  ['which', 'wa', 'of', 'cours', 'the', 'correct', 'answer', 'well', 'done', 'georgina', 'loui', 'braill', 'the', 'inventor', 'of', 'a', 'read', 'system', 'which', 'is', 'known', 'worldwid', 'simpli', 'as', 'braill', 'i', 'suppos', 'browser', 'is', 'an', 'earli', 'exampl', 'of', 'assist', 'technolog', 'system', 'and', 'equip', 'that', 'assist', 'peopl', 'with', 'disabl', 'to', 'perform', 'everyday', 'function', 'let', 'recap', 'the', 'rest', 'of', 'the', 'recoveri', 'neil', 'okay', 'and', 'obstacl', 'is', 'an', 'object', 'that', 'is', 'in', 'your', 'way', 'and', 'block', 'your', 'movement', 'some', 'assist', 'technolog', 'work', 'use', 'echo', 'locat', 'our', 'system', 'of', 'ultrasound', 'detect', 'is', 'use', 'by', 'back', 'be', 'blind', 'is', 'differ', 'from', 'be', 'visual', 'impair', 'have', 'a', 'decreas', 'abil', 'to', 'see', 'whether', 'disabl', 'or', 'not', 'and', 'final', 'the', 'hope', 'is', 'that', 'assist', 'phone', 'us', 'can', 'help', 'normalis', 'disabl', 'chang', 'the', 'percept', 'of', 'someth', 'into', 'be', 'accept', 'as', 'normal', 'so', 'that', 'disabl', 'is', 'no', 'longer', 'a', 'big', 'deal', 'not', 'a', 'big', 'problem', 'that', 'all', 'for', 'thi', 'programm', 'but', 'join', 'us', 'again', 'soon', 'at', 'six', 'minut', 'english', 'and', 'rememb', 'you', 'can', 'find', 'mani', 'more', 'six', 'minut', 'topic', 'and', 'use', 'vocabulari', 'archiv', 'on', 'bbc', 'learn', 'english', 'dot', 'com', 'do', \"n't\", 'forget', 'we', 'also', 'have', 'an', 'app', 'you', 'can', 'download', 'for', 'free', 'from', 'the', 'app', 'store', 'and', 'of', 'cours', 'we', 'are', 'all', 'over', 'social', 'media', 'so', 'come', 'on', 'over', 'and', 'say', 'hi', 'bye', 'for', 'now', 'bye'] \n",
      "\n",
      "Original words  which was of course the correct answer well done Georgina Louis braille the inventor of a reading system which is known worldwide simply as braille I suppose browser is an early example of assistive technology systems and equipment that assist people with disabilities to perform everyday functions let's recap the rest of the recovery Neil okay and obstacle is an object that is in your way and blocks your movement some assistive technology works using echo location our system of ultrasound detection is used by back being blind is different from being visually impaired having a decreased ability to see whether disabling or not and finally the hope is that assisted phone us can help normalise disability change the perception of something into being accepted as normal so that disability is no longer a big deal not a big problem that's all for this programme but join us again soon at six minute English and remember you can find many more six minute topics and useful vocabulary archived on BBC learning English dot com don't forget we also have an app you can download for free from the app stores and of course we are all over social media so come on over and say hi bye for now bye  \n",
      "\n",
      "Sentence Frequenices  {'which': 1, 'wa': 1, 'of': 1, 'cours': 2, 'the': 1, 'correct': 1, 'answer': 1, 'well': 1, 'done': 1, 'georgina': 1, 'loui': 1, 'braill': 2, 'inventor': 1, 'a': 1, 'read': 1, 'system': 3, 'is': 1, 'known': 1, 'worldwid': 1, 'simpli': 1, 'as': 1, 'i': 1, 'suppos': 1, 'browser': 1, 'an': 1, 'earli': 1, 'exampl': 1, 'assist': 4, 'technolog': 2, 'and': 1, 'equip': 1, 'that': 1, 'peopl': 1, 'with': 1, 'disabl': 4, 'to': 1, 'perform': 1, 'everyday': 1, 'function': 1, 'let': 1, 'recap': 1, 'rest': 1, 'recoveri': 1, 'neil': 1, 'okay': 1, 'obstacl': 1, 'object': 1, 'in': 1, 'your': 1, 'way': 1, 'block': 1, 'movement': 1, 'some': 1, 'work': 1, 'use': 3, 'echo': 1, 'locat': 1, 'our': 1, 'ultrasound': 1, 'detect': 1, 'by': 1, 'back': 1, 'be': 1, 'blind': 1, 'differ': 1, 'from': 1, 'visual': 1, 'impair': 1, 'have': 1, 'decreas': 1, 'abil': 1, 'see': 1, 'whether': 1, 'or': 1, 'not': 1, 'final': 1, 'hope': 1, 'phone': 1, 'us': 2, 'can': 1, 'help': 1, 'normalis': 1, 'chang': 1, 'percept': 1, 'someth': 1, 'into': 1, 'accept': 1, 'normal': 1, 'so': 1, 'no': 1, 'longer': 1, 'big': 2, 'deal': 1, 'problem': 1, 'all': 1, 'for': 1, 'thi': 1, 'programm': 1, 'but': 1, 'join': 1, 'again': 1, 'soon': 1, 'at': 1, 'six': 2, 'minut': 2, 'english': 2, 'rememb': 1, 'you': 1, 'find': 1, 'mani': 1, 'more': 1, 'topic': 1, 'vocabulari': 1, 'archiv': 1, 'on': 1, 'bbc': 1, 'learn': 1, 'dot': 1, 'com': 1, 'do': 1, \"n't\": 1, 'forget': 1, 'we': 1, 'also': 1, 'app': 2, 'download': 1, 'free': 1, 'store': 1, 'are': 1, 'over': 1, 'social': 1, 'media': 1, 'come': 1, 'say': 1, 'hi': 1, 'bye': 2, 'now': 1} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "l=processFile(text)\n",
    "for i in l:\n",
    "    print(type(i))\n",
    "    print(\"processedWords \",i.processedWords,\"\\n\")\n",
    "    print(\"Original words \",i.originalWords,\"\\n\")\n",
    "    print(\"Sentence Frequenices \",i.wordFrequencies,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFs(sentences):\n",
    "    tfs = {}\n",
    "    for sent in sentences:\n",
    "        wordFreqs = sent.getWordFreq()\n",
    "        for word in wordFreqs.keys():\n",
    "            if tfs.get(word, 0) != 0:\n",
    "                tfs[word] = tfs[word] + wordFreqs[word]\n",
    "            else:\n",
    "                tfs[word] = wordFreqs[word]\n",
    "    return tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDFs(sentences):\n",
    "    N = len(sentences)\n",
    "    idf = 0\n",
    "    idfs = {}\n",
    "    words = {}\n",
    "    w2 = []\n",
    "    for sent in sentences:\n",
    "        for word in sent.getProcessedWords():\n",
    "            if sent.getWordFreq().get(word, 0) != 0:\n",
    "                words[word] = words.get(word, 0)+ 1\n",
    "    for word in words:\n",
    "        n = words[word]\n",
    "        try:\n",
    "            w2.append(n)\n",
    "            idf = math.log10(float(N)/n)\n",
    "        except ZeroDivisionError:\n",
    "            idf = 0\n",
    "        idfs[word] = idf            \n",
    "    return idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(sentences):\n",
    "    tfs = TFs(sentences)\n",
    "    idfs = IDFs(sentences)\n",
    "    retval = {}\n",
    "    for word in tfs:\n",
    "        tf_idfs=  tfs[word] * idfs[word]\n",
    "        if retval.get(tf_idfs, None) == None:\n",
    "            retval[tf_idfs] = [word]\n",
    "        else:\n",
    "            retval[tf_idfs].append(word)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceSim(sentence1, sentence2, IDF_w):\n",
    "    s1=sentence1.originalWords;\n",
    "    s2=sentence2.originalWords;\n",
    "    print(s1,\" \"s2)\n",
    "    return cosine_similarity(s1,s2)\n",
    "    '''numerator = 0\n",
    "    denominator = 0\n",
    "    for word in sentence2.getProcessedWords():\n",
    "        numerator+= sentence1.getWordFreq().get(word,0) * sentence2.getWordFreq().get(word,0) *  IDF_w.get(word,0) ** 2\n",
    "\n",
    "    for word in sentence1.getProcessedWords():\n",
    "        denominator+= ( sentence1.getWordFreq().get(word,0) * IDF_w.get(word,0) ) ** 2\n",
    "    try:\n",
    "        return numerator / math.sqrt(denominator)\n",
    "    except ZeroDivisionError:\n",
    "        return float(\"-inf\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shreyabanerjee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# Program to measure the similarity between  \n",
    "# two sentences using cosine similarity. \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# tokenization \n",
    "def cosine_similarity(X,Y):\n",
    "    X_list = word_tokenize(X)  \n",
    "    Y_list = word_tokenize(Y) \n",
    "\n",
    "    # sw contains the list of stopwords \n",
    "    sw = stopwords.words('english')  \n",
    "    l1 =[];l2 =[] \n",
    "\n",
    "    # remove stop words from the string \n",
    "    X_set = {w for w in X_list if not w in sw}  \n",
    "    Y_set = {w for w in Y_list if not w in sw} \n",
    "\n",
    "    # form a set containing keywords of both strings  \n",
    "    rvector = X_set.union(Y_set)  \n",
    "    for w in rvector: \n",
    "        if w in X_set: l1.append(1) # create a vector \n",
    "        else: l1.append(0) \n",
    "        if w in Y_set: l2.append(1) \n",
    "        else: l2.append(0) \n",
    "    c = 0\n",
    "\n",
    "    # cosine formula  \n",
    "    for i in range(len(rvector)): \n",
    "            c+= l1[i]*l2[i] \n",
    "    f=float((sum(l1)*sum(l2))**0.5) \n",
    "    if(f!=0):\n",
    "        cosine = c / f\n",
    "    else:\n",
    "        cosine=0\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildQuery(sentences, TF_IDF_w, n):\n",
    "    scores = TF_IDF_w.keys()\n",
    "    scores=sorted(scores,reverse=True)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    sent=\"\"\n",
    "    queryWords = []\n",
    "    while(i<n):\n",
    "        words = TF_IDF_w[scores[j]]\n",
    "        for word in words:\n",
    "            sent=sent+word\n",
    "            queryWords.append(word)\n",
    "            i=i+1\n",
    "            if (i>n): \n",
    "                break\n",
    "        j=j+1\n",
    "    return MySentence(queryWords, queryWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestSentence(sentences, query,IDF):\n",
    "\n",
    "    best_sentence = None\n",
    "    maxVal = float(\"-inf\")\n",
    "    for sent in sentences:\n",
    "        similarity = sentenceSim(sent, query, IDF)\n",
    "        if similarity > maxVal:\n",
    "            best_sentence = sent\n",
    "            maxVal = similarity\n",
    "    sentences.remove(best_sentence)\n",
    "    return best_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSummary(sentences, best_sentence, query, summary_length, lambta, IDF):\n",
    "    summary = [best_sentence]\n",
    "    sum_len = len(best_sentence.getProcessedWords())\n",
    "    MMRval={}\n",
    "    while (sum_len < summary_length):\n",
    "        MMRval={}\n",
    "        for sent in sentences:\n",
    "            mmr= MMRScore(sent, query, summary, lambta, IDF)\n",
    "            MMRval[sent]=mmr\n",
    "        maxxer = max(MMRval, key=MMRval.get)\n",
    "        summary.append(maxxer)\n",
    "        sentences.remove(maxxer)\n",
    "        sum_len += len(maxxer.getProcessedWords())\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMRScore(Si, query, Sj, lambta, IDF):\n",
    "    Sim1 = sentenceSim(Si, query, IDF)\n",
    "    l_expr = lambta * Sim1\n",
    "    value = [float(\"-inf\")]\n",
    "    for sent in Sj:\n",
    "        Sim2 = sentenceSim(Si, sent, IDF)\n",
    "        value.append(Sim2)\n",
    "    r_expr = (1-lambta) * max(value)\n",
    "    MMR_SCORE = l_expr - r_expr\n",
    "    return MMRScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "        sentences=processFile(text)\n",
    "        IDF_w = IDFs(sentences)\n",
    "        TF_IDF_w= TF_IDF(sentences)\n",
    "        TF_w = TFs(sentences)\n",
    "        query = buildQuery(sentences, TF_IDF_w, 100)\n",
    "        best1sentence = bestSentence(sentences, query,IDF_w)\n",
    "        summary = makeSummary(sentences, best1sentence, query ,50, 0.8, IDF_w)\n",
    "        final_summary = \"\"\n",
    "        for sent in summary:\n",
    "            final_summary = final_summary + sent.getOriginalWords() + \"\\n\"\n",
    "        final_summary = final_summary[:-1]\n",
    "        return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from BBC learning English .this is a download from BBC learning English to find out more visit our website .from BBC learning English dot com .hello this is six minutes English from BBC learning English I'm nail and I'm Georgina what do U. Homer Rachel's and Jorge bought has all have in common to a Gina .so that's the agent Greek poet Homer American singer Ray Charles and Argentine writer Jorge Luis Borges I can't see much in common they kneel well the answer is that they were all blind but that obviously didn't hold them back I mean there were some of the greatest artists ever right but I wonder how easy they would find it living and working in the modern world trying people can use a guy dog or a white came to help them move around yes but it won't Kane is hardly advanced technology recently smartphone apps have been invented which dramatically improve the lives of blind people around the world in this programme on blindness in the digital age will be looking at some of these inventions known collectively as assistive technology that's any software or equipment that helps people work around that disabilities or challenges but first it's time for my quick question Georgina in eighteen forty two a technique of using fingers to fill printed raised dots was invented which allowed blind people to read but who invented it was it a Margaret Walker B. Louis braille or C. Samuel Morse I've heard of lost code but that wouldn't help blind people read so I think it's being Louis braille okay Georgina will find out the answer at the end of the programme one remarkable feature of the latest assistive technology is its practicality smartphone apps like be my eyes allow blind users to find lost keys cross busy roads and even colour match their clothes prior no wonder is CEO of a canyon company developing this kind of technology here he explained to BBC world service programme digital planet how his devices situation Hans not replaced the traditional white came .the device is very compatible with any kind of white again so once you click on it anyways can I to work perfectly to detect the obstacles in front of you as usual lies on echo location so I can location is the same technology used by either but and dolphins to detect prey and obstacles and all that you send out our sound faults and then once it both off and also quote you can tell how far the other policies when attached to a white cane the digital device called six cents can detect obstacles objects which block your way making it difficult for you to move forward six cents works using echo location a kind of ultra sound like that used by back who send out sound ways which balance off surrounding objects the returning echoes show where these objects are located some of the assistive apps are so smart they can even tell what kind of object is coming up ahead but a friend a shop door or a speeding car I guess being able to move around confidently really boost people's independence absolutely and it's challenging stereotypes around blindness to log out phone a lower who is blind herself uses assistive apps everyday here she is talking to BBC world service's digital planet .I think the more that society sees blind people in the community at work in relationships it does help to tackle all of these stereotypes it helps to people for feedback blind and visually impaired people in a whole new way and it just normalises disability that's what we need we need to see people just getting on with their life and doing it and then people won't see a such a big deal anymore I'll just be the ordinary .further distinguishes between people who are blind or unable to see I'm those who are visually impaired experience a decreased ability to see .east of tech help blind people leave normal independent lives within their local communities fern hopes that this will help normalise disability treat something as normal which has not been accepted as normal before so being blind it doesn't have to be a big deal an informal way to say something is not a serious problem just keep your eyes closed for a minute and try moving around the room you'll soon see how difficult it is and how life changing this technology can be being able to read books must also open up a world of imagination so what was the answer to your question now %HESITATION yes I asked Georgina who invented the system of reading west finger tips are used to fill patterns of printed raise dot what did you say Georgina I thought it was B. Louis braille .which was of course the correct answer well done Georgina Louis braille the inventor of a reading system which is known worldwide simply as braille I suppose browser is an early example of assistive technology systems and equipment that assist people with disabilities to perform everyday functions let's recap the rest of the recovery Neil okay and obstacle is an object that is in your way and blocks your movement some assistive technology works using echo location our system of ultrasound detection is used by back being blind is different from being visually impaired having a decreased ability to see whether disabling or not and finally the hope is that assisted phone us can help normalise disability change the perception of something into being accepted as normal so that disability is no longer a big deal not a big problem that's all for this programme but join us again soon at six minute English and remember you can find many more six minute topics and useful vocabulary archived on BBC learning English dot com don't forget we also have an app you can download for free from the app stores and of course we are all over social media so come on over and say hi bye for now bye .from BBC learning English \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-cd94d0511a07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-258-6047c0bd77ea>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mTF_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTF_IDF_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mbest1sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbestSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIDF_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeSummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest1sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDF_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfinal_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-255-edf426a611e0>\u001b[0m in \u001b[0;36mbestSentence\u001b[0;34m(sentences, query, IDF)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmaxVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentenceSim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msimilarity\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxVal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mbest_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-fa570c4bc77a>\u001b[0m in \u001b[0;36msentenceSim\u001b[0;34m(sentence1, sentence2, IDF_w)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ms1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginalWords\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ms2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginalWords\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     '''numerator = 0\n\u001b[1;32m      6\u001b[0m     \u001b[0mdenominator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-a95f189fbc03>\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mX_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mY_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# sw contains the list of stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \"\"\"\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \"\"\"\n\u001b[1;32m   1356\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after_tok\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "summary=process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so that's the agent Greek poet Homer American singer Ray Charles and Argentine writer Jorge Luis Borges I can't see much in common they kneel well the answer is that they were all blind but that obviously didn't hold them back I mean there were some of the greatest artists ever right but I wonder how easy they would find it living and working in the modern world trying people can use a guy dog or a white came to help them move around yes but it won't Kane is hardly advanced technology recently smartphone apps have been invented which dramatically improve the lives of blind people around the world in this programme on blindness in the digital age will be looking at some of these inventions known collectively as assistive technology that's any software or equipment that helps people work around that disabilities or challenges but first it's time for my quick question Georgina in eighteen forty two a technique of using fingers to fill printed raised dots was invented which allowed blind people to read but who invented it was it a Margaret Walker B\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
